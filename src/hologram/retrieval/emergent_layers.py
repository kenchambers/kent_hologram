"""
EmergentLayerManager: Dynamically create and route to semantic layers.

Pattern: MemoryTrace.store_with_surprise() (novelty detection) +
         IntentClassifier (prototype learning) +
         Resonator (description generation)

Key innovation: Uses surprise-gated resonance to decide layer creation.
"""

import time
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
import uuid

import torch

from hologram.core.operations import Operations
from hologram.core.similarity import Similarity
from hologram.persistence.faiss_adapter import FaissAdapter


@dataclass
class SemanticLayer:
    """
    A dynamically emerged semantic layer.
    
    Attributes:
        layer_id: Unique identifier
        description: Human-readable description (generated by resonator)
        prototype_vec: Bundled prototype vector (Hebbian)
        faiss_index: Layer-specific fact storage
        fact_count: Number of facts in layer
        created_at: Timestamp of creation
        last_accessed: Timestamp of last access
    """
    layer_id: str
    description: str
    prototype_vec: torch.Tensor
    faiss_index: FaissAdapter
    fact_count: int
    created_at: float
    last_accessed: float


@dataclass
class LayerRoutingResult:
    """
    Result of routing decision.
    
    Attributes:
        layer: Selected or newly created layer
        is_new: Whether this is a newly created layer
        surprise: Surprise score (novelty)
        confidence: Routing confidence
    """
    layer: SemanticLayer
    is_new: bool
    surprise: float
    confidence: float


class EmergentLayerManager:
    """
    Manages dynamically emerging semantic layers.
    
    Pattern: Surprise-gated layer creation using existing novelty detection
    
    Thresholds:
    - CREATE_THRESHOLD = 0.7: High surprise → create new layer
    - ROUTE_THRESHOLD = 0.3: Low surprise → route to existing layer
    - MERGE_THRESHOLD = 0.9: Very similar → merge layers
    
    Methods:
        route_or_create: Route content to layer or create new one
        create_layer: Create new semantic layer
        strengthen_layer: Hebbian strengthening of layer prototype
        merge_similar_layers: Prevent layer explosion
    """
    
    # Thresholds from plan
    CREATE_THRESHOLD = 0.7  # High surprise → new layer
    ROUTE_THRESHOLD = 0.3   # Low surprise → existing layer
    MERGE_THRESHOLD = 0.9   # Very similar → merge
    
    def __init__(
        self,
        dimensions: int,
        persist_base_path: str,
        use_hnsw: bool = True,
    ):
        """
        Initialize layer manager.
        
        Args:
            dimensions: Vector dimensionality
            persist_base_path: Base path for layer persistence
            use_hnsw: Use HNSW for layer indices
        """
        self._dimensions = dimensions
        self._persist_base_path = persist_base_path
        self._use_hnsw = use_hnsw
        self._layers: Dict[str, SemanticLayer] = {}
    
    def route_or_create(
        self,
        content_vec: torch.Tensor,
        content_text: str,
        description_generator=None,
    ) -> LayerRoutingResult:
        """
        Route content to appropriate layer or create new one.
        
        Pattern: MemoryTrace.store_with_surprise() novelty detection
        
        Args:
            content_vec: Content vector to route
            content_text: Text description for layer generation
            description_generator: Optional LayerDescriptionGenerator
        
        Returns:
            LayerRoutingResult with routing decision
        """
        # No layers yet → create first one
        if not self._layers:
            layer = self.create_layer(
                content_vec,
                content_text,
                description_generator,
            )
            return LayerRoutingResult(
                layer=layer,
                is_new=True,
                surprise=1.0,
                confidence=1.0,
            )
        
        # Compute similarity to all layer prototypes
        similarities = []
        for layer_id, layer in self._layers.items():
            sim = Similarity.cosine(content_vec, layer.prototype_vec)
            similarities.append((layer_id, sim))
        
        # Find best match
        best_layer_id, max_sim = max(similarities, key=lambda x: x[1])
        surprise = 1.0 - max_sim
        
        # High surprise → create new layer
        if surprise > self.CREATE_THRESHOLD:
            layer = self.create_layer(
                content_vec,
                content_text,
                description_generator,
            )
            return LayerRoutingResult(
                layer=layer,
                is_new=True,
                surprise=surprise,
                confidence=1.0 - surprise,
            )
        
        # Low surprise → route to existing layer
        layer = self._layers[best_layer_id]
        layer.last_accessed = time.time()
        
        return LayerRoutingResult(
            layer=layer,
            is_new=False,
            surprise=surprise,
            confidence=max_sim,
        )
    
    def create_layer(
        self,
        content_vec: torch.Tensor,
        content_text: str,
        description_generator=None,
    ) -> SemanticLayer:
        """
        Create new semantic layer.
        
        Args:
            content_vec: Initial content vector
            content_text: Text for description generation
            description_generator: Optional LayerDescriptionGenerator
        
        Returns:
            Newly created SemanticLayer
        """
        # Generate unique ID
        layer_id = str(uuid.uuid4())
        
        # Generate description
        if description_generator:
            description = description_generator.generate_description([content_text])
        else:
            # Fallback: use first 50 chars of content
            description = content_text[:50] + ("..." if len(content_text) > 50 else "")
        
        # Create FAISS index for this layer
        faiss_path = f"{self._persist_base_path}/layer_{layer_id}"
        faiss_index = FaissAdapter(
            dimensions=self._dimensions,
            persist_path=faiss_path,
            use_hnsw=self._use_hnsw,
        )
        
        # Create layer
        layer = SemanticLayer(
            layer_id=layer_id,
            description=description,
            prototype_vec=content_vec.clone(),
            faiss_index=faiss_index,
            fact_count=0,
            created_at=time.time(),
            last_accessed=time.time(),
        )
        
        # Store layer
        self._layers[layer_id] = layer
        
        return layer
    
    def strengthen_layer(
        self,
        layer_id: str,
        content_vec: torch.Tensor,
        learning_rate: float = 0.1,
    ) -> None:
        """
        Strengthen layer prototype with Hebbian bundling.
        
        Pattern: IntentClassifier.learn() Hebbian learning
        
        Args:
            layer_id: Layer to strengthen
            content_vec: New content vector
            learning_rate: Learning rate for bundling
        """
        if layer_id not in self._layers:
            return
        
        layer = self._layers[layer_id]
        
        # Hebbian bundling: prototype ← bundle(prototype, content)
        layer.prototype_vec = Operations.bundle(
            layer.prototype_vec,
            content_vec * learning_rate,
        )
        
        # Normalize to prevent drift
        norm = torch.norm(layer.prototype_vec)
        if norm > 1e-6:
            layer.prototype_vec = layer.prototype_vec / norm
        
        layer.last_accessed = time.time()
    
    def get_layer(self, layer_id: str) -> Optional[SemanticLayer]:
        """Get layer by ID."""
        return self._layers.get(layer_id)
    
    def get_all_layers(self) -> List[SemanticLayer]:
        """Get all layers."""
        return list(self._layers.values())
    
    def get_layer_descriptions(self) -> List[Tuple[str, str]]:
        """
        Get all layer descriptions for semantic search.
        
        Returns:
            List of (layer_id, description) tuples
        """
        return [(lid, layer.description) for lid, layer in self._layers.items()]
    
    def merge_similar_layers(
        self,
        threshold: float = None,
    ) -> int:
        """
        Merge layers with high prototype similarity.
        
        Prevents layer explosion by combining very similar layers.
        
        Args:
            threshold: Similarity threshold for merging (default: MERGE_THRESHOLD)
        
        Returns:
            Number of layers merged
        """
        if threshold is None:
            threshold = self.MERGE_THRESHOLD
        
        merged_count = 0
        layer_ids = list(self._layers.keys())
        
        # Check all pairs
        for i in range(len(layer_ids)):
            for j in range(i + 1, len(layer_ids)):
                layer_a_id = layer_ids[i]
                layer_b_id = layer_ids[j]
                
                # Skip if already merged
                if layer_a_id not in self._layers or layer_b_id not in self._layers:
                    continue
                
                layer_a = self._layers[layer_a_id]
                layer_b = self._layers[layer_b_id]
                
                # Check similarity
                sim = Similarity.cosine(layer_a.prototype_vec, layer_b.prototype_vec)
                
                if sim > threshold:
                    # Merge B into A
                    layer_a.prototype_vec = Operations.bundle(
                        layer_a.prototype_vec,
                        layer_b.prototype_vec,
                    )
                    
                    # Normalize
                    norm = torch.norm(layer_a.prototype_vec)
                    if norm > 1e-6:
                        layer_a.prototype_vec = layer_a.prototype_vec / norm
                    
                    # Update metadata
                    layer_a.fact_count += layer_b.fact_count
                    layer_a.description = f"{layer_a.description} & {layer_b.description}"
                    
                    # Remove B
                    del self._layers[layer_b_id]
                    merged_count += 1
        
        return merged_count
    
    def get_stats(self) -> Dict:
        """Get layer statistics."""
        return {
            "total_layers": len(self._layers),
            "total_facts": sum(layer.fact_count for layer in self._layers.values()),
            "avg_facts_per_layer": (
                sum(layer.fact_count for layer in self._layers.values()) / len(self._layers)
                if self._layers else 0
            ),
        }
    
    def __repr__(self) -> str:
        return f"EmergentLayerManager(layers={len(self._layers)})"
